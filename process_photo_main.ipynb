{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c55bdcd-76eb-49b8-bb99-968b9f872d8c",
   "metadata": {},
   "source": [
    "### **Препроцессинг фотковых текстов**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432a5ad-e3f8-4f91-a818-c4d52c77aec6",
   "metadata": {},
   "source": [
    "#### **1. Начальная обработка**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3388b-7100-4410-b0e7-6ace39df671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_file = \"recognized_all_gpt.txt\"\n",
    "output_file = \"recognized_all_gpt_new.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r'-\\s*\\n\\s*', '', text)\n",
    "text = re.sub(r'\\n+', ' ', text)\n",
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "sentences = re.split(r'(?<=[.!?])(?:\\s+|(?=[\"“”\\'»)])|(?=\\Z))', text)\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sentences))\n",
    "\n",
    "print(f\"Количество предложений: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28fc01-77cb-466a-b598-d5a923ebfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Очищает текст от всех знаков препинания, кроме точки, и удаляет цифры.\n",
    "    Каждое исходное предложение остается на новой строке.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()  # Разделяем текст на строки (предложения)\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        line = re.sub(r\"[!\\\"#$%&'()*+,-\\/:;<=>?@\\[\\\\\\]^_`{|}~–«»…]+\", \"\", line)\n",
    "        line = re.sub(r\"\\d+\", \"\", line)\n",
    "\n",
    "        # удаляем лишние пробелы\n",
    "        line = re.sub(r\" +\", \" \", line)\n",
    "        line = line.strip()\n",
    "\n",
    "\n",
    "        if line:\n",
    "            if not line.endswith('.'):\n",
    "                 if line:\n",
    "                      line += '.'\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "# Имена файлов\n",
    "input_filename = 'recognized_all_gpt_new.txt'\n",
    "output_filename = 'not_punct.txt'\n",
    "\n",
    "try:\n",
    "\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
    "        original_text = infile.read()\n",
    "\n",
    "\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(cleaned_text)\n",
    "\n",
    "    print(f\"Файл '{input_filename}' успешно обработан.\")\n",
    "    print(f\"Результат сохранен в файл '{output_filename}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Файл '{input_filename}' не найден.\")\n",
    "    print(\"Пожалуйста, убедитесь, что файл находится в той же папке, что и скрипт, или укажите правильный путь.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при обработке файла: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07148f28-5adb-49ed-b6e7-88cc4d07eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_revised(text):\n",
    "    \"\"\"\n",
    "    Очищение текста:\n",
    "    1. Удаление последовательности 'd''' и 'l'''.\n",
    "    2. Только буквы (из любого алфавита) и пробелы.\n",
    "    3. Нет цифр и пунктуации.\n",
    "    4. Нормальные пробелы\n",
    "    Каждая исходная строка остается на новой строке в результате.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        processed_line = line.replace(\"d'\", \"\")\n",
    "        processed_line = processed_line.replace(\"l'\", \"\")\n",
    "\n",
    "        # Оставить толкьо буквы и пробелы\n",
    "        filtered_chars = [char for char in processed_line if char.isalpha() or char.isspace()]\n",
    "        filtered_line = \"\".join(filtered_chars)\n",
    "        # всё с одним пробелом \n",
    "        final_line = re.sub(r'\\s+', ' ', filtered_line)\n",
    "        final_line = final_line.strip()\n",
    "\n",
    "        # Добавляем непустые строки\n",
    "        if final_line:\n",
    "            cleaned_lines.append(final_line)\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "input_filename = 'recognized_all_gpt_new.txt'\n",
    "output_filename = 'not_punct.txt'\n",
    "\n",
    "try:\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
    "        original_text = infile.read()\n",
    "\n",
    "    cleaned_text = clean_text_revised(original_text)\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(cleaned_text)\n",
    "\n",
    "    print(f\"Файл '{input_filename}' успешно обработан.\")\n",
    "    print(f\"Текст после удаления 'd\\'', 'l\\'', цифр и пунктуации сохранен в файл '{output_filename}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Файл '{input_filename}' не найден.\")\n",
    "    print(\"Что-то не так с путём\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при обработке файла: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcb6af-2be8-4992-bb9d-24c3cc8695f1",
   "metadata": {},
   "source": [
    "#### **2. Сохранение именованных сущностей (топонимы) в отдельный файл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4899c-60dc-450d-878e-5ebe4f54a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_entities(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    entities = set()\n",
    "    sentences = text.split('\\n')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "        for i, word in enumerate(words):\n",
    "            #считаем именованной сущностью слово, если оно начинается с заглавной латинской буквы\n",
    "            if len(word) > 1 and word[0].isupper() and word.isascii():\n",
    "                # фильтр по длине\n",
    "                if len(word) > 2:\n",
    "                    entities.add(word)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(sorted(entities)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'not_punct.txt'          \n",
    "    output_file = 'named_entities.txt'\n",
    "\n",
    "    find_entities(input_file, output_file)\n",
    "    print(f'Результаты в {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354e2d0-5c7c-46f5-a5ab-f79b0b8a5be5",
   "metadata": {},
   "source": [
    "#### **3. Лемматизация файла с помощью парсера urmi-parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406bcb8-24cf-4fbb-a20a-eb3e94fe8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniparser_urmi import UrmiAnalyzer\n",
    "\n",
    "def tokenize_and_lemmatize(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Токенизирует и лемматизирует предложения из файла.\n",
    "    Если лемма не найдена для слова, начинающегося на \"в\", удаляет \"в\" и пробует ещё раз.\n",
    "    \"\"\"\n",
    "    a = UrmiAnalyzer(mode='nodiacritics')\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        sentences = [line.strip() for line in infile if line.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentence = []\n",
    "        for word in sentence.split():\n",
    "            analyses = a.analyze_words(word)\n",
    "            lemmatized_word_analyses = []\n",
    "            lemma_found = False\n",
    "            \n",
    "            for ana in analyses:\n",
    "                if hasattr(ana, 'lemma') and ana.lemma:\n",
    "                    lemmatized_word_analyses.append({\n",
    "                        \"token\": ana.wf if hasattr(ana, 'wf') else word,\n",
    "                        \"lemma\": ana.lemma,\n",
    "                        \"gramm\": ana.gramm if hasattr(ana, 'gramm') else \"\",\n",
    "                    })\n",
    "                    lemma_found = True\n",
    "                    break\n",
    "            \n",
    "            if not lemma_found and word.lower().startswith('в') and len(word) > 1:\n",
    "                modified_word = word[1:]\n",
    "                analyses_mod = a.analyze_words(modified_word)\n",
    "                for ana in analyses_mod:\n",
    "                    if hasattr(ana, 'lemma') and ana.lemma:\n",
    "                        lemmatized_word_analyses.append({\n",
    "                            \"token\": ana.wf if hasattr(ana, 'wf') else modified_word,\n",
    "                            \"lemma\": ana.lemma,\n",
    "                            \"gramm\": ana.gramm if hasattr(ana, 'gramm') else \"\",\n",
    "                        })\n",
    "                        lemma_found = True\n",
    "                        break\n",
    "            \n",
    "            if not lemma_found:\n",
    "                lemmatized_word_analyses.append({\n",
    "                    \"token\": word,\n",
    "                    \"lemma\": \"\",\n",
    "                    \"gramm\": \"\", \n",
    "                })\n",
    "            \n",
    "            lemmatized_sentence.append({\n",
    "                \"word\": word,\n",
    "                \"analyses\": lemmatized_word_analyses,\n",
    "            })\n",
    "        \n",
    "        results.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"lemmatized_words\": lemmatized_sentence,\n",
    "        })\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for item in results:\n",
    "            outfile.write(f\"Предложение: {item['sentence']}\\n\")\n",
    "            for word_info in item[\"lemmatized_words\"]:\n",
    "                outfile.write(f\"  Слово: {word_info['word']}\\n\")\n",
    "                for analysis in word_info[\"analyses\"]:\n",
    "                    outfile.write(f\"    Токен: {analysis.get('token', '')}\\n\") \n",
    "                    outfile.write(f\"      Лемма: {analysis.get('lemma', '')}\\n\")\n",
    "                    outfile.write(f\"      Грамматика: {analysis.get('gramm', '')}\\n\")\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'not_punct.txt'\n",
    "    output_file = 'lemmatized_not_punct_all.txt'\n",
    "    \n",
    "    print(f\"Запуск токенизации и лемматизации для файла '{input_file}'...\")\n",
    "    tokenize_and_lemmatize(input_file, output_file)\n",
    "    print(f\"Обработка завершена. Результаты в '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df2190-2f06-4c3a-8b9a-29f8cf76f048",
   "metadata": {},
   "source": [
    "запись нелемматизированных слов в отдельный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba8b89-10fc-4208-b463-a7b5827a68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_without_lemma(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Файл с not_lemma\n",
    "    Находит и записывает в файл слова без леммы (не короче 4 символов) из файла output_lemmatized_all.txt.\n",
    "    \"\"\"\n",
    "\n",
    "    words_without_lemma = []\n",
    "    current_word = None\n",
    "    lemma_found = False\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith(\"Слово:\"):\n",
    "                if current_word and not lemma_found and len(current_word) >= 4:\n",
    "                    words_without_lemma.append(current_word)\n",
    "                current_word = line.split(\": \", 1)[1]\n",
    "                lemma_found = False\n",
    "            elif line.startswith(\"Лемма:\") and current_word:\n",
    "                parts = line.split(\": \", 1)\n",
    "                if len(parts) > 1:\n",
    "                    lemma = parts[1].strip()\n",
    "                    if lemma:\n",
    "                         lemma_found = True\n",
    "\n",
    "    if current_word and not lemma_found and len(current_word) >= 4:\n",
    "         words_without_lemma.append(current_word)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word in words_without_lemma:\n",
    "            outfile.write(word + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'lemmatized_not_punct_all.txt'\n",
    "    output_file = 'not_lemma_with_photo.txt'\n",
    "    find_words_without_lemma(input_file, output_file)\n",
    "    print(f\"Слова без леммы (не короче 4 символов) записаны в файл '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c25a5-57f0-4db7-9727-cb02ea24c685",
   "metadata": {},
   "source": [
    "#### **4. Лемматизация нелемматизированного остатка с fuzzywuzzy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d34200-2175-4e04-848e-2d038e64c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fuzzywuzzy import process\n",
    "from uniparser_urmi import UrmiAnalyzer\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_all_urmi_lexemes(lexemes_file):\n",
    "    \"\"\"\n",
    "    Читает лексемы из файла словаря урми и возвращает список лексем.\n",
    "    \"\"\"\n",
    "    lexemes = set() # set для быстрой проверки наличия\n",
    "    try:\n",
    "        with open(lexemes_file, 'r', encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                if line.strip().startswith(\"lex: \"):\n",
    "                    lexeme = line.strip().replace('lex: ', '')\n",
    "                    lexemes.add(lexeme)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл лексем '{lexemes_file}' не найден.\")\n",
    "        return None # если ошибка\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при чтении файла лексем '{lexemes_file}': {e}\")\n",
    "        return None\n",
    "    return list(lexemes) y\n",
    "\n",
    "def extract_consonants(word):\n",
    "    \"\"\"\n",
    "    извлекает последовательность согласных из слова.\n",
    "    \"\"\"\n",
    "    return \"\".join(re.findall(r\"[bcdfghjklmnpqrstvwxyz’вƶşçţqk]\", word, re.IGNORECASE))\n",
    "\n",
    "def compare_consonant_sequences(original_word, lexeme):\n",
    "    \"\"\"\n",
    "    Сравнивает последовательности согласных ОРИГИНАЛЬНОГО слова и лексемы.\n",
    "    Возвращает коэффициент похожести (0-100).\n",
    "    \"\"\"\n",
    "    word_cons = extract_consonants(original_word)\n",
    "    lexeme_cons = extract_consonants(lexeme)\n",
    "    if not word_cons or not lexeme_cons:\n",
    "        return 0\n",
    "\n",
    "    total_weight = 0\n",
    "    weighted_match = 0\n",
    "    word_idx = 0\n",
    "    lexeme_idx = 0\n",
    "\n",
    "    while word_idx < len(word_cons) and lexeme_idx < len(lexeme_cons):\n",
    "        weight = 1.0\n",
    "        if word_idx < 2: weight = 3.0\n",
    "        elif word_idx < 3: weight = 2.0\n",
    "\n",
    "        total_weight += weight\n",
    "\n",
    "        if word_cons[word_idx].lower() == lexeme_cons[lexeme_idx].lower(): # сравнение без учета регистра\n",
    "            weighted_match += weight\n",
    "            lexeme_idx += 1\n",
    "        word_idx += 1\n",
    "\n",
    "    return (weighted_match / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "\n",
    "def find_best_lemmas(word, urmi_lexemes, limit=10, threshold=60):\n",
    "    \"\"\"\n",
    "    Находит наиболее вероятные леммы для слова, используя нечеткое сравнение и сравнение последовательности согласных.\n",
    "    Пробует варианты с заменой q - k\n",
    "    Возвращает список кортежей (лемма, оценка_согласных).\n",
    "    \"\"\"\n",
    "    if not urmi_lexemes: # Если лексемы не загрузились\n",
    "        return []\n",
    "\n",
    "    word_lower = word.lower()\n",
    "    word_variations = {word} # Начинаем с оригинального слова\n",
    "\n",
    "\n",
    "    \n",
    "    #генерируем варианты с заменой q - k\n",
    "    if 'q' in word_lower:\n",
    "        variant_k = ''.join(['k' if c == 'q' else 'K' if c == 'Q' else c for c in word])\n",
    "        word_variations.add(variant_k)\n",
    "    if 'k' in word_lower:\n",
    "        variant_q = ''.join(['q' if c == 'k' else 'Q' if c == 'K' else c for c in word])\n",
    "        word_variations.add(variant_q)\n",
    "\n",
    "\n",
    "    \n",
    "    combined_results = {} #cловарь для хранения лучших результатов {лемма: лучшаяоценкасогласных}\n",
    "\n",
    "    # Ищем (оригинал + q/k замены)\n",
    "    for variant in word_variations:\n",
    "        # 1й отбор с помощью fuzzywuzzy для текущего вара ( с увелич лимита)\n",
    "        preliminary_matches = process.extract(variant, urmi_lexemes, limit=limit * 2)\n",
    "        if not preliminary_matches:\n",
    "            continue\n",
    "\n",
    "        # 2. сравнение согласных ОРИГИНАЛЬНОГО слова и выбор подходящих лемм\n",
    "        for lexeme, fuzzy_score in preliminary_matches:\n",
    "            # Сравниваем последовательность согласных ОРИГИНАЛЬНОГО слова с лексемой\n",
    "            consonant_score = compare_consonant_sequences(word, lexeme)\n",
    "\n",
    "            # Фильтр по порогу согласных и длине\n",
    "            if consonant_score >= threshold and len(lexeme) >= len(word) / 3:\n",
    "                #сохраняем или обновляем результат, если текущая оценка лучше\n",
    "                combined_results[lexeme] = max(combined_results.get(lexeme, 0), consonant_score)\n",
    "\n",
    "    sorted_matches = sorted(combined_results.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Возвращаем топ N результатов после объединения и сортировки\n",
    "    return sorted_matches[:limit]\n",
    "\n",
    "\n",
    "def find_missing_lemmas(my_words_file, lexemes_file, output_file, limit=10, threshold=60):\n",
    "    \"\"\"\n",
    "    Находит и предлагает леммы для слов, которые не смог распознать UrmiAnalyzer и записывает результаты в файл\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analyzer = UrmiAnalyzer(mode='nodiacritics')\n",
    "    except Exception as e:\n",
    "        print(f\"Критическая ошибка: Не удалось инициализировать UrmiAnalyzer: {e}\")\n",
    "        return\n",
    "\n",
    "    urmi_lexemes = get_all_urmi_lexemes(lexemes_file)\n",
    "    if urmi_lexemes is None: # Проверка на лексемы\n",
    "        print(\"Не удалось загрузить лексемы.\")\n",
    "        return\n",
    "\n",
    "    urmi_lexemes_set = set(urmi_lexemes) # Используем set для быстрой проверки наличия\n",
    "\n",
    "    try:\n",
    "        with open(my_words_file, 'r', encoding='utf-8') as infile, \\\n",
    "             open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "            for line in infile:\n",
    "                word = line.strip()\n",
    "                if not word: continue # Пропускаем пустые строки\n",
    "\n",
    "\n",
    "                \n",
    "                try:\n",
    "                    analyses = analyzer.analyze_words(word)\n",
    "                    has_lemma = False\n",
    "                    found_lemma_str = \"\" # строка для хранения найденной леммы\n",
    "                    if analyses:\n",
    "                        for ana in analyses:\n",
    "                            if hasattr(ana, 'lemma') and ana.lemma:\n",
    "                                has_lemma = True\n",
    "                                found_lemma_str = ana.lemma # запомнили первую найденную лемму\n",
    "                                break # достаточно одной найденной леммы\n",
    "\n",
    "                    # Если парсер нашёл лемму, записываем\n",
    "                    if has_lemma:\n",
    "                         outfile.write(f\"Слово: {word}, Найдена лемма (UrmiAnalyzer): {found_lemma_str}\\n\")\n",
    "                         continue # переход к следующему слову\n",
    "\n",
    "                    # если не нашёл\n",
    "                    best_lemmas = find_best_lemmas(word, urmi_lexemes, limit, threshold)\n",
    "                    if best_lemmas:\n",
    "                        # вывод с оценкой согл\n",
    "                        lemmas_output = ', '.join([f'{lex} ({score:.1f}%)' for lex, score in best_lemmas])\n",
    "                        outfile.write(f\"Слово: {word}, Предложенные леммы (по согласным): {lemmas_output}\\n\")\n",
    "                    else:\n",
    "                        outfile.write(f\"Слово: {word}, Не найдено подходящих лемм\\n\")\n",
    "\n",
    "                except Exception as e_word:\n",
    "                    print(f\"Ошибка при обработке слова '{word}': {e_word}\")\n",
    "                    outfile.write(f\"Слово: {word}, Ошибка обработки: {e_word}\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл со словами '{my_words_file}' не найден.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при работе с файлами: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_words_file = \"not_lemma_with_photo.txt\"\n",
    "    lexemes_file = \"lexemes.txt\"                \n",
    "    output_file = \"prob_lemmatized_not_lemma_photo_all.txt\" \n",
    "\n",
    "    print(\"Запуск поиска недостающих лемм...\")\n",
    "    find_missing_lemmas(my_words_file, lexemes_file, output_file, limit=10, threshold=60)\n",
    "    print(f\"Поиск завершен. Результаты сохранены в '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383bae8f-e847-4573-9946-d802a6379554",
   "metadata": {},
   "source": [
    "#### **5. Расфасовка на отлемматизированные слова и всё ещё ошибки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e19d6-ca87-4056-adfc-c144699dd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подчищенный\n",
    "\n",
    "import re\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# глобальные строчные константы\n",
    "VOWELS = \"aeiouаеёиоуыэюя\"\n",
    "CONSONANTS = set(\"bcdfghjklmnpqrstvwxyzбвгджзйклмнпрстфхцчшщ\")\n",
    "\n",
    "# ФУНКЦИИ\n",
    "\n",
    "# для извлечения гласных из слова\n",
    "def extract_vowels(word):\n",
    "    return \"\".join([c.lower() for c in word if c.lower() in VOWELS])\n",
    "\n",
    "# для расчета схожести гласных\n",
    "def vowel_similarity(vowels1, vowels2):\n",
    "    set1 = set(vowels1)\n",
    "    set2 = set(vowels2)\n",
    "    if not vowels1 or not vowels2: return 0.0\n",
    "    common_unique_vowels = len(set1.intersection(set2))\n",
    "    min_len = min(len(vowels1), len(vowels2))\n",
    "    return (common_unique_vowels / min_len) * 100 if min_len > 0 else 0.0\n",
    "\n",
    "# для удаления дубликатов строк\n",
    "def remove_duplicate_lines(input_file, output_file):\n",
    "    unique_lines = set()\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            for line in infile: unique_lines.add(line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Не найден файл: {input_file}\")\n",
    "        return False\n",
    "    \n",
    "    sorted_lines = sorted(list(unique_lines))\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for line in sorted_lines:\n",
    "                if line: outfile.write(line + '\\n')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка записи: {output_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "# ОСНОВНОЙ\n",
    "# для подсчета несовпадающих символов\n",
    "def count_mismatched_chars(word, lemma):\n",
    "    word_chars = set(word)\n",
    "    return sum(1 for char in lemma if char not in word_chars)\n",
    "\n",
    "# для выбора оптимальной леммы\n",
    "def choose_best_lemma(word, lemmas_str):\n",
    "    lemmas = []\n",
    "    pattern = re.compile(r\"([^\\s()]+)\\s*\\((\\d+\\.?\\d*)%\\)\")\n",
    "    \n",
    "    if not lemmas_str:\n",
    "        return None\n",
    "\n",
    "    #парсинг строки с вариантами лемм\n",
    "    for item in lemmas_str.split(\", \"):\n",
    "        match = pattern.search(item.strip())\n",
    "        if match:\n",
    "            lemma_text = match.group(1)\n",
    "            try: score = float(match.group(2))\n",
    "            except ValueError: continue\n",
    "            lemmas.append((lemma_text, score))\n",
    "\n",
    "    if not lemmas: return None\n",
    "\n",
    "    #Выбор леммы с макс %\n",
    "    try:\n",
    "        max_score = max(score for _, score in lemmas)\n",
    "        best_lemmas = [(lemma, score) for lemma, score in lemmas if score == max_score]\n",
    "        \n",
    "        if len(best_lemmas) == 1:\n",
    "            return best_lemmas[0]\n",
    "            \n",
    "        # проверка если с одинаковым %\n",
    "        return min(best_lemmas, key=lambda item: count_mismatched_chars(word, item[0]))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ПРОЦЕСС\n",
    "\n",
    "#основная функция обработки файла с леммами\n",
    "def process_lemmatized_file(input_file, all_file, problem_file):\n",
    "    processed_count = 0\n",
    "    all_count = 0\n",
    "    problem_count = 0\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "             open(all_file, 'w', encoding='utf-8') as allout, \\\n",
    "             open(problem_file, 'w', encoding='utf-8') as probout:\n",
    "\n",
    "            # каждая строка\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                processed_count += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "\n",
    "                # разные типы строк\n",
    "                if re.match(r\"Слово:.*,\\s*Найдена лемма \\(UrmiAnalyzer\\):\", line) or \\\n",
    "                   re.match(r\"Слово:.*,\\s*Найдена лемма \\(точное совпадение\\):\", line) or \\\n",
    "                   (\"Предложенная лемма:\" in line and \"Предложенные леммы\" not in line):\n",
    "                    allout.write(line + '\\n')\n",
    "                    all_count += 1\n",
    "                    continue\n",
    "                elif \"Не найдено подходящих лемм\" in line:\n",
    "                    probout.write(line + '\\n')\n",
    "                    problem_count += 1\n",
    "                    continue\n",
    "\n",
    "                #Обработка предложенных лемм\n",
    "                elif \"Предложенные леммы (по согласным):\" in line:\n",
    "                    # Парсинг слова и вариантов лемм\n",
    "                    match_word = re.search(r\"Слово:\\s*([^\\s,]+),\", line)\n",
    "                    if not match_word:\n",
    "                        probout.write(line + ' (Ошибка парсинга слова)\\n')\n",
    "                        problem_count += 1\n",
    "                        continue\n",
    "                    word = match_word.group(1)\n",
    "\n",
    "                    # Доп обработка\n",
    "                    match_lemmas = re.search(r\"Предложенные леммы \\(по согласным\\):\\s*(.+)\", line)\n",
    "                    if not match_lemmas:\n",
    "                         probout.write(line + ' (Ошибка парсинга лемм)\\n')\n",
    "                         problem_count += 1\n",
    "                         continue\n",
    "\n",
    "                    # выбор финальной леммы\n",
    "                    lemmas_str = match_lemmas.group(1)\n",
    "                    final_lemma_tuple = None\n",
    "\n",
    "                    # Спец обработка для слов с определенными ПРЕФИКСАМИ\n",
    "                    if word and len(word) > 1 and word[0].lower() in 'dlbв' and word[1].lower() in CONSONANTS:\n",
    "                        modified_word = word[1:]\n",
    "                        alternative_lemma_tuple = choose_best_lemma(modified_word, lemmas_str)\n",
    "                        final_lemma_tuple = alternative_lemma_tuple if alternative_lemma_tuple else choose_best_lemma(word, lemmas_str)\n",
    "                    else:\n",
    "                        final_lemma_tuple = choose_best_lemma(word, lemmas_str)\n",
    "\n",
    "                    #Проверка выбранной леммы  \n",
    "                    if final_lemma_tuple:\n",
    "                       best_lemma, score = final_lemma_tuple\n",
    "\n",
    "                       #Проверка гласных для больших %\n",
    "                       if score >= 80:\n",
    "                           word_vowels = extract_vowels(word)\n",
    "                           lemma_vowels = extract_vowels(best_lemma)\n",
    "                           sim = vowel_similarity(word_vowels, lemma_vowels)\n",
    "                           vowel_count_diff = len(word_vowels) - len(lemma_vowels)\n",
    "\n",
    "                           # фильтрует по гласным\n",
    "                           if sim >= 25 and vowel_count_diff <= 3:\n",
    "                               allout.write(f\"Слово: {word}, Предложенная лемма: {best_lemma} ({score:.1f}%)\\n\")\n",
    "                               all_count += 1\n",
    "                           else:\n",
    "                               reason = f\"совпадение гласных {sim:.1f}%\"\n",
    "                               if vowel_count_diff > 3: reason += f\", разница гласных {vowel_count_diff}\"\n",
    "                               probout.write(f\"Слово: {word}, Лемма: {best_lemma} ({score:.1f}%), не совпало по гласным ({reason})\\n\")\n",
    "                               problem_count += 1\n",
    "\n",
    "                       # Обработывает низкокачественные леммы\n",
    "                       elif score < 63 and len(word) > 6:\n",
    "                           probout.write(f\"Слово: {word}, Низкая уверенность: {best_lemma} ({score:.1f}%)\\n\")\n",
    "                           problem_count += 1\n",
    "                       else:\n",
    "                           allout.write(f\"Слово: {word}, Предложенная лемма: {best_lemma} ({score:.1f}%)\\n\")\n",
    "                           all_count += 1\n",
    "                    else:\n",
    "                        probout.write(f\"Слово: {word}, Ошибка выбора леммы из: {lemmas_str}\\n\")\n",
    "                        problem_count += 1\n",
    "\n",
    "                else:\n",
    "                    probout.write(line + ' (Неопознанный формат)\\n')\n",
    "                    problem_count += 1\n",
    "\n",
    "        print(f\"\\nОбработано строк: {processed_count}\")\n",
    "        print(f\"Успешные леммы: {all_count}\")\n",
    "        print(f\"Проблемные случаи: {problem_count}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл не найден: {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Критическая ошибка: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_raw_file = \"prob_lemmatized_not_lemma_photo_all.txt\"\n",
    "    unique_file = \"prob_lemmatized_not_lemma_photo_unique.txt\"\n",
    "    all_file = \"prob_lemmatized_photo_all_final.txt\"\n",
    "    problem_file = \"prob_lemmatized_photo_problem.txt\"\n",
    "\n",
    "    if not os.path.exists(input_raw_file):\n",
    "      print(f\"Ошибка: Файл не найден: {input_raw_file}\")\n",
    "      exit(1)\n",
    "\n",
    "    print(f\"Удаление дубликатов...\")\n",
    "    if remove_duplicate_lines(input_raw_file, unique_file):\n",
    "        print(f\"Обработка файла...\")\n",
    "        process_lemmatized_file(unique_file, all_file, problem_file)\n",
    "        print(f\"\\nРезультаты:\")\n",
    "        print(f\"Успешные леммы: {all_file}\")\n",
    "        print(f\"Проблемные случаи: {problem_file}\")\n",
    "    else:\n",
    "        print(\"Ошибка обработки\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342977a-4416-4941-8d33-6f762554ea8b",
   "metadata": {},
   "source": [
    "#### **6. Объединение проблемных слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e70bb9-6d87-4383-841e-77646ee2771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher\n",
    "import os \n",
    "\n",
    "input_problem_file = 'prob_lemmatized_photo_problem.txt'\n",
    "output_file = 'photo_problems.txt' \n",
    "\n",
    "# для вычисления похожести строк\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio() * 100\n",
    "\n",
    "# Читка проблем и их лемм из файла\n",
    "def read_problem_words(input_file):\n",
    "    problem_words = []\n",
    "    pattern_word = re.compile(r\"Слово: ([^,]+),\")\n",
    "    pattern_lemma = re.compile(r\"Лемма: ([^\\(]+) \\((\\d+\\.?\\d*)%\\)\")\n",
    "    pattern_line = re.compile(r\"Слово: ([^,]+), Лемма: ([^\\(]+) \\((\\d+\\.?\\d*)%\\),?\")\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                #поиск слова и леммы\n",
    "                m_word = pattern_word.search(line)\n",
    "                if m_word:\n",
    "                    word = m_word.group(1)\n",
    "                    lemmas = []\n",
    "                    m_lemma = pattern_lemma.findall(line)\n",
    "                    if m_lemma:\n",
    "                        for lm in m_lemma:\n",
    "                            lemmas.append((lm[0].strip(), float(lm[1])))\n",
    "                    else:\n",
    "                        # Если леммы нет, поискать в след строке\n",
    "                        m_line = pattern_line.search(line)\n",
    "                        if m_line:\n",
    "                            word = m_line.group(1)\n",
    "                            lemmas = [(m_line.group(2).strip(), float(m_line.group(3)))]\n",
    "                        else:\n",
    "                            lemmas = []\n",
    "                    problem_words.append({\"word\": word, \"lemmas\": lemmas, \"line\": line})\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл '{input_file}' не найден.\")\n",
    "        return None # если ошибка\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при чтении файла '{input_file}': {e}\")\n",
    "        return None\n",
    "\n",
    "    return problem_words\n",
    "\n",
    "\n",
    "#Объединение схожих слов!\n",
    "def cluster_similar_words(problem_words, similarity_threshold=70):\n",
    "    clusters = []\n",
    "    used_indices = set()\n",
    "\n",
    "    for i, pw1 in enumerate(problem_words):\n",
    "        if i in used_indices:\n",
    "            continue\n",
    "        cluster = [pw1]\n",
    "        used_indices.add(i)\n",
    "        for j, pw2 in enumerate(problem_words[i+1:], start=i+1):\n",
    "            if j in used_indices:\n",
    "                continue\n",
    "            sim = similarity(pw1['word'], pw2['word'])\n",
    "            if sim >= similarity_threshold:\n",
    "                cluster.append(pw2)\n",
    "                used_indices.add(j)\n",
    "        clusters.append(cluster)\n",
    "    return clusters\n",
    "\n",
    "# f чтобы выбрать наиболее частотную или по первой по алфавиту форму\n",
    "def choose_representative(cluster):\n",
    "    words = [item['word'] for item in cluster]\n",
    "    freq = Counter(words)\n",
    "    max_freq = max(freq.values())\n",
    "    candidates = [w for w, c in freq.items() if c == max_freq]\n",
    "    return candidates[0]\n",
    "\n",
    "# Объединение лемм из кластера\n",
    "def merge_lemmas(cluster):\n",
    "    lemma_scores = defaultdict(float)\n",
    "    for item in cluster:\n",
    "        for lemma, score in item['lemmas']:\n",
    "            # сохранять макс оценку для леммы\n",
    "            lemma_scores[lemma] = max(lemma_scores[lemma], score)\n",
    "    # Сортирует леммы по УБЫВАНИЮ оценки (т.е сначала лучшие)\n",
    "    sorted_lemmas = sorted(lemma_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_lemmas\n",
    "\n",
    "# ДЛЯ ИТОГ записи\n",
    "def format_results(clusters):\n",
    "    merged_results = []\n",
    "    for cluster in clusters:\n",
    "        rep_word = choose_representative(cluster)\n",
    "        merged_lemmas = merge_lemmas(cluster)\n",
    "        lemmas_str = \", \".join([f\"{lemma} ({score:.1f}%)\" for lemma, score in merged_lemmas])\n",
    "        merged_results.append(f\"Слово: {rep_word}, Предложенные леммы: {lemmas_str}\")\n",
    "    return merged_results\n",
    "\n",
    "\n",
    "# ОСНОВ ЧАСТЬ\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(input_problem_file):\n",
    "        print(f\"Ошибка: Файл '{input_problem_file}' не найден.\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Чтение данных из '{input_problem_file}'...\")\n",
    "    problem_words = read_problem_words(input_problem_file)\n",
    "\n",
    "    if problem_words is None: # на ошибку\n",
    "        print(\"Чтение файла прервано из-за ошибки.\")\n",
    "        exit(1)\n",
    "\n",
    "    # кластеризуем схожие слова\n",
    "    print(\"Кластеризация схожих слов...\")\n",
    "    clusters = cluster_similar_words(problem_words)\n",
    "\n",
    "    # формируем резы\n",
    "    print(\"Формирование результатов...\")\n",
    "    formatted_results = format_results(clusters)\n",
    "\n",
    "    # запись в файл\n",
    "    print(f\"Запись результатов в '{output_file}'...\")\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for line in formatted_results:\n",
    "                outfile.write(line + '\\n')\n",
    "\n",
    "        print(f\"Объединенные результаты сохранены в файл '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл '{output_file}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e3da8-90c1-453f-b7e0-4f02d61a15c8",
   "metadata": {},
   "source": [
    "#### **7. Восстановление предложений**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0abb5e8-0458-4e93-8dec-47a7a8aeba28",
   "metadata": {},
   "source": [
    "(с удалением имен. сущ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab83b4-b8ac-455e-9ad1-1efbdf1c3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# почищенный от комментов\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# Именов сущ \n",
    "NAMED_ENTITIES_FILE = 'named_entities.txt'\n",
    "named_entities = set()\n",
    "\n",
    "try:\n",
    "    with open(NAMED_ENTITIES_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entity = line.strip()\n",
    "            if entity:\n",
    "                named_entities.add(entity.lower())  # Нормализация для сравнения\n",
    "        print(f\"Загружено {len(named_entities)} уникальных сущностей.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки сущностей: {e}\")\n",
    "\n",
    "\n",
    "input_main_file = 'lemmatized_not_punct_all.txt'\n",
    "prob_final_file = 'prob_lemmatized_photo_all_final.txt' \n",
    "problems_clustered_file = 'photo_problems.txt'\n",
    "output_final_sentences_file = 'output_lemmas_sentences_final_new.txt'\n",
    "\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"вычисление процентной схожести строк\"\"\"\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() * 100 if a and b else 0\n",
    "\n",
    "def load_prob_final_lemmas(filename):\n",
    "    \"\"\" парсинг файла с предобработанными леммами\"\"\"\n",
    "    lemma_map = {}\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                match = re.search(r\"Слово:\\s*([^\\s,]+)\\s*,\\s*Предложенная лемма:\\s*([^\\s\\(]+)\", line)\n",
    "                if match:\n",
    "                    lemma_map[match.group(1).lower()] = match.group(2).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка чтения лемм: {e}\")\n",
    "    return lemma_map\n",
    "\n",
    "def load_clustered_problem_words(filename):\n",
    "    \"\"\" извлечение проблемных слов для fuzzywuzzy-поиска\"\"\"\n",
    "    word_list = []\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                match = re.search(r\"Слово:\\s*([^\\s,]+)\\s*,\", line)\n",
    "                if match:\n",
    "                    word_list.append(match.group(1))\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка чтения проблемных слов: {e}\")\n",
    "    return word_list\n",
    "\n",
    "def find_similar_word(target_word, word_list, threshold=70):\n",
    "    \"\"\" поиск приблизительного совпадения с приоритетом строчных вариантов\"\"\"\n",
    "    best_match = None\n",
    "    for word in word_list:\n",
    "        if len(word) > 3 and similarity(target_word, word) >= threshold:\n",
    "            # Предпочтение к нижнему регистру\n",
    "            if not best_match or word[0].islower():\n",
    "                best_match = word\n",
    "                if word[0].islower():\n",
    "                    break  # Лучший вар найден\n",
    "    return best_match\n",
    "\n",
    "# Процесс\n",
    "def apply_fallback_logic(word, token, prob_lemmas, problem_words, is_first_word):\n",
    "    \"\"\"иерархический выбор оптимальной леммы\"\"\"\n",
    "    if not word:\n",
    "        return token or \"\"  # защита от пустых значений\n",
    "\n",
    "    #  1: Именованные сущности\n",
    "    if word.lower() in named_entities:\n",
    "        # Удаление сущностей с заглавной буквы\n",
    "        return None if word[0].isupper() else word\n",
    "    \n",
    "    #  2: Предобработанные леммы\n",
    "    lemma = prob_lemmas.get(word.lower())\n",
    "    if lemma:\n",
    "        return word if word[0].isupper() else lemma\n",
    "    \n",
    "    # 3: Нечеткий поиск в проблемных словах\n",
    "    similar = find_similar_word(word.lower() if is_first_word else word, problem_words)\n",
    "    if similar:\n",
    "        return similar\n",
    "    \n",
    "    # 4-5: Токен без префикса или исходное слово\n",
    "    return token or word\n",
    "\n",
    "# ОСНОВА\n",
    "print(\" Инициализация обработки \")\n",
    "start_time = time.time()\n",
    "\n",
    "prob_final_lemmas = load_prob_final_lemmas(prob_final_file)\n",
    "clustered_problem_words = load_clustered_problem_words(problems_clustered_file)\n",
    "\n",
    "results = []\n",
    "current_context = {\n",
    "    \"text\": None,\n",
    "    \"lemmas\": [],\n",
    "    \"current_word\": None,\n",
    "    \"current_token\": None,\n",
    "    \"is_first_word\": False\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(input_main_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Обработка предложений\n",
    "            if line.startswith(\"Предложение:\"):\n",
    "                if current_context[\"text\"] and current_context[\"lemmas\"]:\n",
    "                    results.append({\n",
    "                        \"sentence\": current_context[\"text\"], \n",
    "                        \"lemmas\": current_context[\"lemmas\"]\n",
    "                    })\n",
    "                current_context = {\n",
    "                    \"text\": line.split(\": \", 1)[1],\n",
    "                    \"lemmas\": [],\n",
    "                    \"current_word\": None,\n",
    "                    \"current_token\": None,\n",
    "                    \"is_first_word\": True\n",
    "                }\n",
    "\n",
    "            # Обработка слов\n",
    "            elif line.startswith(\"Слово:\"):\n",
    "                if current_context[\"current_word\"]:\n",
    "                    lemma = apply_fallback_logic(\n",
    "                        current_context[\"current_word\"],\n",
    "                        current_context[\"current_token\"],\n",
    "                        prob_final_lemmas,\n",
    "                        clustered_problem_words,\n",
    "                        current_context[\"is_first_word\"]\n",
    "                    )\n",
    "                    if lemma is not None:\n",
    "                        current_context[\"lemmas\"].append(lemma)\n",
    "                current_context[\"current_word\"] = line.split(\": \", 1)[1]\n",
    "                current_context[\"current_token\"] = None\n",
    "\n",
    "            # Обработка токенов\n",
    "            elif line.startswith(\"Токен:\") and current_context[\"current_word\"]:\n",
    "                current_context[\"current_token\"] = line.split(\": \", 1)[1] or current_context[\"current_word\"]\n",
    "\n",
    "            # Финализация леммы\n",
    "            elif line.startswith(\"Лемма:\") and current_context[\"current_word\"]:\n",
    "                lemma = line.split(\": \", 1)[1].split('/', 1)[0].strip()\n",
    "                final_lemma = lemma if len(lemma) >= 4 else apply_fallback_logic(\n",
    "                    current_context[\"current_word\"],\n",
    "                    current_context[\"current_token\"],\n",
    "                    prob_final_lemmas,\n",
    "                    clustered_problem_words,\n",
    "                    current_context[\"is_first_word\"]\n",
    "                )\n",
    "                if final_lemma is not None:\n",
    "                    current_context[\"lemmas\"].append(final_lemma)\n",
    "                current_context[\"current_word\"] = None\n",
    "\n",
    "    # Финализация последнего предложения\n",
    "    if current_context[\"text\"] and current_context[\"lemmas\"]:\n",
    "        results.append({\n",
    "            \"sentence\": current_context[\"text\"], \n",
    "            \"lemmas\": current_context[\"lemmas\"]\n",
    "        })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Критическая ошибка: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Резы\n",
    "try:\n",
    "    with open(output_final_sentences_file, 'w', encoding='utf-8') as outfile:\n",
    "        for item in results:\n",
    "            outfile.write(' '.join(item['lemmas']) + '\\n')\n",
    "    print(f\"Успешно сохранено {len(results)} предложений.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка сохранения: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"Общее время выполнения: {time.time()-start_time:.2f} сек.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5940c9d-33b3-4518-b635-4bbda4ec665a",
   "metadata": {},
   "source": [
    "#### **8. Конечная очистка и подсчёт слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12994e35-0ecd-41ca-9b8c-e405ca6f292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import traceback \n",
    "\n",
    "def filter_lemmas_and_sentences(input_file, output_file):\n",
    "    filtered_lines = []\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Ошибка: Входной файл '{input_file}' не найден.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    VOWELS_LOWER = set('aeiou')\n",
    "    LATIN_CONSONANTS_LOWER = set(string.ascii_lowercase) - VOWELS_LOWER\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            for line_num, line in enumerate(infile, 1): \n",
    "                words = line.strip().split()\n",
    "                processed_words = []\n",
    "\n",
    "                for word in words:\n",
    "                    current_word = word.split('/', 1)[0].strip() if '/' in word else word\n",
    "\n",
    "                    if not current_word: continue \n",
    "                    current_word_lower = current_word.lower()\n",
    "\n",
    "                    word_to_check = current_word_lower \n",
    "                    prefix_removed = False \n",
    "\n",
    "                    if len(current_word_lower) >= 3 and current_word_lower.startswith(('d', 'l')):\n",
    "                        char1 = current_word_lower[1]\n",
    "                        char2 = current_word_lower[2]\n",
    "                        if char1 in LATIN_CONSONANTS_LOWER and char2 in LATIN_CONSONANTS_LOWER:\n",
    "                            current_word_lower = current_word_lower[1:] # префикс\n",
    "                            prefix_removed = True\n",
    "\n",
    "                    if len(current_word_lower) >= 3:\n",
    "                        processed_words.append(current_word_lower)\n",
    "\n",
    "\n",
    "                # количество слов в предложении \n",
    "                if len(processed_words) > 2:\n",
    "                    filtered_lines.append(\" \".join(processed_words) + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при чтении файла '{input_file}' (строка ~{line_num}): {e}\")\n",
    "        traceback.print_exc() # Добавим вывод стека ошибок\n",
    "        return\n",
    "\n",
    "    # рез в файле\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.writelines(filtered_lines)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл '{output_file}': {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'output_lemmas_sentences_final_new.txt'\n",
    "    output_file = 'photo_sentences_filtered_new.txt'\n",
    "\n",
    "    filter_lemmas_and_sentences(input_file, output_file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "         if os.path.getsize(output_file) > 0:\n",
    "              print(f\"Результаты записаны в {output_file}\")\n",
    "         else:\n",
    "              print(f\"Выходной файл {output_file} создан, но он пуст (нет подходящих строк).\")\n",
    "    else:\n",
    "         print(f\"Выходной файл {output_file} не был создан (возможно, из-за ошибки).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a924e5b-21f6-4f27-a3b5-0460829a5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_unique_words(input_file):\n",
    "    \"\"\"\n",
    "    Читает файл и считает количество уникальных слов.\n",
    "    \"\"\"\n",
    "    unique_words = set()\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return len(unique_words)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'photo_sentences_filtered_new.txt'\n",
    "    unique_word_count = count_unique_words(input_file)\n",
    "    print(f\"Количество уникальных слов в файле {input_file}: {unique_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0946ca-449c-443b-9276-bbba5de4214b",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________\n",
    "слов уникальных - 4719 \n",
    "\n",
    "предложений - 3445\n",
    "_____________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b393e-0e03-4190-a482-e70eabe02cc2",
   "metadata": {},
   "source": [
    "#### **9. Объединение художественных и газетных текстов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861920a-8237-47a1-bae9-4d40f367f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение предложений из двух файлов в один\n",
    "\n",
    "input_file1 = \"photo_sentences_filtered_new.txt\"\n",
    "input_file2 = \"output_lemmas_sentences_filtered.txt\"\n",
    "output_file = \"general_sentences_filtered.txt\"\n",
    "\n",
    "def merge_sentence_files(file1, file2, outfile):\n",
    "    sentences = []\n",
    "    for fname in [file1, file2]:\n",
    "        try:\n",
    "            with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        sentences.append(line)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при чтении файла {fname}: {e}\")\n",
    "    try:\n",
    "        with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "            for sent in sentences:\n",
    "                out.write(sent + \"\\n\")\n",
    "        print(f\"Объединено {len(sentences)} предложений. Результат записан в {outfile}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл {outfile}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_sentence_files(input_file1, input_file2, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745cba7-5085-42f4-b4ee-01c9b7fb3295",
   "metadata": {},
   "source": [
    "### **Модель, обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b32f6-045b-4897-85ec-e53127a8926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_corpus(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    documents = []\n",
    "    for line in fin:\n",
    "        documents.append(line.split())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc019df-58a4-43a8-a2b2-a19afae5cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта функция сохраняет словарь (где ключи - слова, а значения - их векторные представления) в файл.\n",
    "# Это необходимо, чтобы сохранить результаты обучения модели.\n",
    "def save_dictionary(fname, dictionary, args):\n",
    "    length, dimension = args # длина словаря и размерность векторов\n",
    "    fin = io.open(fname, 'w', encoding='utf-8')\n",
    "    fin.write('%d %d\\n' % (length, dimension))\n",
    "    for word in dictionary:\n",
    "        fin.write('%s %s\\n' % (word, ' '.join(map(str, dictionary[word]))))\n",
    "\n",
    "# Эта функция читает словарь из файла, созданного с помощью функции save_dictionary.\n",
    "# Это позволяет загрузить ранее обученную модель.\n",
    "def load_dictionary(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    length, dimension = map(int, fin.readline().split())\n",
    "    dictionary = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        dictionary[tokens[0]] = map(float, tokens[1:])\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11830fd9-2acf-4ea2-a987-b56115fced3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_corpus('general_sentences_filtered.txt')\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923ad85-1023-49d1-b774-b0eb7f02eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa1d22-aecb-48cc-a654-737d65359ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b122a2-8ce0-4aa3-8a5f-98bab9d9d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#dimension = 8 # размерность векторов слов (эмбеддингов).\n",
    "#model = Word2Vec(sentences=documents, vector_size=dimension, min_count=1)    было 50 вектор-сайз\n",
    "\n",
    "model2 = Word2Vec(\n",
    "    sentences=documents,\n",
    "    vector_size=dimension,       # размерность\n",
    "    window=3,            # размер окна\n",
    "    min_count=1,         # минимальная частота слова\n",
    "    sg=0,              # CBOW\n",
    "    negative=15,          # negative sampling\n",
    "    alpha=0.025,         # learning rate\n",
    "    min_alpha=0.0001,     # Минимал learning rate\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff99ff7-d175-49d0-afea-5ed896f0af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создает словарь dictionary, где ключами являются слова, а значениями - их векторные представления.\n",
    "dictionary = {key : model2.wv[key] for key in model2.wv.key_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b824c7-d9b1-4abc-996f-48bd15dff6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4ee1c-0bd4-40f2-9177-96ca7cdf5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar('çətun', topn = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419eb438-920e-44c7-b46e-5398cfe7be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary('urmi_dictionary_general.txt', dictionary, (len(dictionary), dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8648c-4976-4c9c-95f8-8ee63ed63a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dictionary = load_dictionary('urmi_dictionary_general.txt')\n",
    "len(dictionary) == len(loaded_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b8900-f2a0-447c-b1f6-ba6923bb973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import traceback \n",
    "\n",
    "def remove_first_line(filepath):\n",
    "    \"\"\"\n",
    "    Удаляет первую строку из указанного файла, тк там прописана размерность векторов\n",
    "    Создает временный файл, копирует все строки, кроме первой, а затем заменяет исходный файл временным.\n",
    "    \"\"\"\n",
    "    print(f\"Попытка удалить первую строку из файла: {filepath}\")\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"ОШИБКА: Файл '{filepath}' не найден.\")\n",
    "        return False\n",
    "\n",
    "    # Создание имени для временного файла  .tmp \n",
    "    temp_filepath = filepath + \".tmp\"\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as infile, \\\n",
    "             open(temp_filepath, 'w', encoding='utf-8') as outfile:\n",
    " \n",
    "            first_line = infile.readline()\n",
    "\n",
    "            if not first_line and os.path.getsize(filepath) == 0:\n",
    "                 print(f\"Предупреждение: Файл '{filepath}' пуст. Ничего не удалено.\")\n",
    "                 return True\n",
    "\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "\n",
    "        # замена исходного файла временным\n",
    "        # проверка на кучу ошибок\n",
    "        try:\n",
    "            os.replace(temp_filepath, filepath)\n",
    "            print(f\"Первая строка успешно удалена из файла '{filepath}'.\")\n",
    "            return True\n",
    "        except OSError as e:\n",
    "            print(f\"ОШИБКА при замене файла '{filepath}' временным файлом '{temp_filepath}': {e}\")\n",
    "            if os.path.exists(temp_filepath):\n",
    "                try:\n",
    "                    os.remove(temp_filepath)\n",
    "                except OSError:\n",
    "                    print(f\"Не удалось удалить временный файл '{temp_filepath}' после ошибки замены.\")\n",
    "            return False\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ОШИБКА: Файл '{filepath}' не найден во время обработки.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ОШИБКА при обработке файла '{filepath}': {e}\")\n",
    "        traceback.print_exc()\n",
    "        if os.path.exists(temp_filepath):\n",
    "            try:\n",
    "                os.remove(temp_filepath)\n",
    "                print(f\"Временный файл '{temp_filepath}' удален после ошибки.\")\n",
    "            except OSError:\n",
    "                 print(f\"Не удалось удалить временный файл '{temp_filepath}' после ошибки.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TARGET_FILENAME = \"urmi_dictionary_general.txt\"\n",
    "\n",
    "    if remove_first_line(TARGET_FILENAME):\n",
    "        print(\"Операция завершена успешно.\")\n",
    "    else:\n",
    "        print(\"Операция завершилась с ошибкой.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf9608-1829-44ee-82e4-68cb6cb8515a",
   "metadata": {},
   "source": [
    "#### **Автоматизированный перевод дырок**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760e464-3542-4cbb-9a01-a51f7b3d7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "def load_translations(lexemes_filepath=\"lexemes.txt\"):\n",
    "    \"\"\"\n",
    "    Загружает переводы (trans_ru) из файла lexemes.txt в словарь.\n",
    "    Ключи словаря - формы слов из поля 'lex:' в нижнем регистре.\n",
    "    \"\"\"\n",
    "    translations = {}\n",
    "    \n",
    "    if not os.path.exists(lexemes_filepath):\n",
    "        print(f\"ОШИБКА: Файл '{lexemes_filepath}' не найден.\")\n",
    "        return translations \n",
    "        \n",
    "    print(f\"Загрузка переводов из файла: {lexemes_filepath}\")\n",
    "\n",
    "    # регулярки для извлечения\n",
    "    lex_pattern = re.compile(r\"^\\s*lex:\\s*(.+)$\")\n",
    "    trans_ru_pattern = re.compile(r\"^\\s*trans_ru:\\s*(.+)$\")\n",
    "\n",
    "    current_lexemes = []\n",
    "    current_trans_ru = None\n",
    "    line_num = 0\n",
    "\n",
    "    \n",
    "    try:\n",
    "        with open(lexemes_filepath, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line_stripped = line.strip()\n",
    "\n",
    "                # начало записи\n",
    "                if line_stripped.startswith(\"-lexeme\"):\n",
    "                    # сохранение прошлой записи, если она окей\n",
    "                    if current_lexemes and current_trans_ru:\n",
    "                        for lex_form in current_lexemes:\n",
    "                            # сохранение ключа в нижнем регистре\n",
    "                            translations[lex_form.lower()] = current_trans_ru\n",
    "                    #сброс для новой записи\n",
    "                    current_lexemes = []\n",
    "                    current_trans_ru = None\n",
    "                    continue \n",
    "                    \n",
    "                #поиск поля 'lex:'\n",
    "                lex_match = lex_pattern.match(line_stripped)\n",
    "                if lex_match and not current_lexemes: # пока не нашли поиск\n",
    "                    lex_value = lex_match.group(1).strip()\n",
    "                    variations = [v.strip() for v in lex_value.split('/') if v.strip()]\n",
    "                    current_lexemes.extend(variations)\n",
    "                    continue\n",
    "\n",
    "                #поиск поля 'trans_ru:'\n",
    "                trans_ru_match = trans_ru_pattern.match(line_stripped)\n",
    "                if trans_ru_match and not current_trans_ru: \n",
    "                    current_trans_ru = trans_ru_match.group(1).strip()\n",
    "                    continue\n",
    "\n",
    "            # сохраняем последнее\n",
    "            if current_lexemes and current_trans_ru:\n",
    "                for lex_form in current_lexemes:\n",
    "                    translations[lex_form.lower()] = current_trans_ru\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ОШИБКА при чтении/парсинге файла '{lexemes_filepath}' (строка ~{line_num}): {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"Загружено переводов для {len(translations)} уникальных форм слов.\")\n",
    "    return translations\n",
    "\n",
    "\n",
    "def translate_lines_keep_original(input_text, translations_dict):\n",
    "    \"\"\"\n",
    "    Принимает текст и словарь переводов.\n",
    "    Заменяет слова (разделенные ' | ') на их переводы или оставляет слово как есть.\n",
    "    Добавляет пустую строку между результатами.\n",
    "    \"\"\"\n",
    "    output_lines = []\n",
    "    lines = input_text.strip().split('\\n') # Разбиваем на строки\n",
    "\n",
    "    for line in lines:\n",
    "        # Разбиение строки на слова по '|' и минус пробелы\n",
    "        original_words = [word.strip() for word in line.split('|') if word.strip()]\n",
    "        translated_or_original_words = []\n",
    "\n",
    "        for word in original_words:\n",
    "            # поиск перевода в словаре\n",
    "            # Если не найдено, то ОРИГИНАЛЬНОЕ СЛОВО (word)\n",
    "            processed_word = translations_dict.get(word.lower(), word)\n",
    "            translated_or_original_words.append(processed_word)\n",
    "\n",
    "        # собираем строку обратно с разделителем ' | '\n",
    "        if translated_or_original_words:\n",
    "             output_lines.append(\" | \".join(translated_or_original_words))\n",
    "\n",
    "    return \"\\n\\n\".join(output_lines) # пустая строка между резами\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_string = \"\"\"\n",
    "zukzikkə | məgxuki | вorş | məsruji\n",
    "ləmsistan | dlitton | prezident | dkomunisti\n",
    "ruznamь | gavo | qarxa | dvilə\n",
    "darəçə | vudun | bkişjutə | bǝşəjuvь\n",
    "vədta | dkule | kəpitalistetə | dqapital\n",
    "krьsţjana | mşijxəjə | qəddijşi | mədxərtə\n",
    "səmt | xzuri | pelxə | tupənq | supraxana\n",
    "məkupi | paşuţь | ţrapa | cjələ\n",
    "ruznamь | qənunə | ktivjə | bcirij\n",
    "bxela | pijşɩ | bijovilə | lkis\n",
    "xləjbə | snədtə | şəxlipə | industrializatsija\n",
    "cərcərtə | nvəxə | niqo | kəlвə\n",
    "вrьjra | miko | nьgaranuta | ptiltə\n",
    "xьjjal | ţmara | pikkir | вləsə\n",
    "məqdənə | şotapaja | dməxnəqtə | maqţalta\n",
    "dakrəj | sovxoze | sənjənə | dkolxoz\n",
    "hoğə | avtomobijl | qujrəti | kombarь | domna\n",
    "agrotexnikə | bjulpana | texnikə | tusə\n",
    "sajjarajь | stakan | рənsil | vərəqə\n",
    "tuç | miljuni | snetə | bəlpi\n",
    "\"\"\"\n",
    "\n",
    "    # файл с лексемами и переводами\n",
    "    lexemes_filename = \"lexemes.txt\"\n",
    "    #сюда переводы..\n",
    "    translations_map = load_translations(lexemes_filename)\n",
    "\n",
    "    if translations_map:\n",
    "        result_text = translate_lines_keep_original(input_string, translations_map)\n",
    "        print(\"\\n--- Результат замены ---\")\n",
    "        print(result_text)\n",
    "    else:\n",
    "        print(\"\\nЗамена не выполнена из-за ошибки загрузки переводов.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

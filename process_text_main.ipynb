{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86136c6-9e7a-468b-924a-52fd3ee35ccb",
   "metadata": {},
   "source": [
    "## **Предпроцессинг (художественной) литературы и CBOW для ассирийского**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f0ca7-11a3-444c-8dff-bf83bdb8624f",
   "metadata": {},
   "source": [
    "\n",
    "    объединить все файлы найденные в один\n",
    "    сделать так, чтобы на каждой строчке по предложению\n",
    "    к нижнему регистру\n",
    "    убрать знаки препинания и лишнее всё, что будет\n",
    "    убрать стоп слова/ короткие слова\n",
    "    токенизировать\n",
    "    лемматизировать (через лемматизатор)\n",
    "    перепроверить, почистить остатки\n",
    "\n",
    "    создать корпус и словарь\n",
    "    cbow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650539fb-c897-4571-89d8-51b3ee017d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install uniparser-urmi\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8829699-e371-45f2-895c-2cc7a7fe5690",
   "metadata": {},
   "source": [
    "#### **1. Объединяем файлы в один, где каждое предложение на отдельной строчке.**\n",
    "- удаляем пунктуацию\n",
    "- всё приводим к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48044caa-bfa4-459f-9f54-b5204de613e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Удаляет пунктуацию из текста, оставляя слова с апострофом.\n",
    "    Если апостроф - второй символ в слове, заменяет его пробелом.\n",
    "    \"\"\"\n",
    "    def replace_apostrophe(match):\n",
    "        word = match.group(0)\n",
    "        if len(word) > 1 and word[1] == '’':   # Обрабатывает '\n",
    "            return word[0] + \" \" + word[2:]\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    pattern = r\"\\b\\w*’\\w*\\b\"\n",
    "    text = re.sub(pattern, replace_apostrophe, text)\n",
    "    return re.sub(r'[^\\w\\s’]', ' ', text)\n",
    "\n",
    "def combine_files_into_sentences_1(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Объединяет текстовые файлы из папки в один файл,\n",
    "    где каждое предложение находится на новой строке.\n",
    "    Разделение на предложения происходит по знакам .?!\n",
    "    \"\"\"\n",
    "\n",
    "    all_text = \"\"\n",
    "    for filename in os.listdir(folder_path): # из списка всех файлов в папке*\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(folder_path, filename) # создаёт путь к файлу\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    all_text += file.read()\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Ошибка кодировки при чтении файла: {filename}. Пожалуйста, убедитесь, что файл в кодировке UTF-8\")\n",
    "                continue\n",
    "                \n",
    "    # Разделение на предложения по знакам . ? !\n",
    "    all_text = all_text.replace('\\n', ' ') # для стихов*\n",
    "    sentences = re.findall(r'[^.?!]+[.?!]?', all_text)\n",
    "    #print(sentences)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for sentence in sentences:\n",
    "            cleaned_sentence = remove_punctuation(sentence) # удаляем пунктуацию после того, как разбили на предложения\n",
    "            lower_sentence = cleaned_sentence.lower()\n",
    "            print(lower_sentence)\n",
    "            outfile.write(lower_sentence.strip() + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  folder = '/home/kuklelik/Jupyter/ассирийские файлы/TXT_оригиналы/'\n",
    "  output_file = 'output_all.txt'\n",
    "  combine_files_into_sentences_1(folder, output_file)\n",
    "  print(f\"Файлы объединены в '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e082103-315c-4667-a626-0fa849036368",
   "metadata": {},
   "source": [
    "#### **2.1 Сохраняем полные разборы предложений (отдельные разобранные слова)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a52a5-2fa1-4250-9856-8bddcf40496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from uniparser_urmi import UrmiAnalyzer\n",
    "import logging\n",
    "\n",
    "# Настройка логгирования\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Токенизирует и лемматизирует предложения из файла.\n",
    "    Если лемма не найдена для слова, начинающегося на \"в\",\n",
    "    удаляет \"в\" и пробует ещё раз.\n",
    "    \"\"\"\n",
    "\n",
    "    a = UrmiAnalyzer(mode='nodiacritics')\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                sentences.append(line)\n",
    "\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            lemmatized_sentence = []\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                analyses = a.analyze_words(word)\n",
    "                lemmatized_words = []\n",
    "                lemma_found = False  # Флаг для отслеживания найденной леммы\n",
    "\n",
    "                for ana in analyses:\n",
    "                    if ana.lemma:  # если лемма не пустая, добавляем анализ\n",
    "                        lemmatized_words.append({\n",
    "                            \"token\": ana.wf,\n",
    "                            \"lemma\": ana.lemma,\n",
    "                            \"gramm\": ana.gramm,\n",
    "                        })\n",
    "                        lemma_found = True\n",
    "                        break\n",
    "\n",
    "                if not lemma_found and word.lower().startswith('в'):  # Проверка в\n",
    "                    modified_word = word[1:]\n",
    "                    analyses = a.analyze_words(modified_word)\n",
    "                    for ana in analyses:\n",
    "                        if ana.lemma:\n",
    "                            lemmatized_words.append({\n",
    "                                \"token\": ana.wf,\n",
    "                                \"lemma\": ana.lemma,\n",
    "                                \"gramm\": ana.gramm,\n",
    "                            })\n",
    "                            lemma_found = True\n",
    "                            break\n",
    "                \n",
    "                if not lemma_found: # если лемма так и не найдена, выводим пустую лемму\n",
    "                    lemmatized_words.append({\n",
    "                                \"token\": word,\n",
    "                                \"lemma\": \"\", # пустая лемма\n",
    "                                \"gramm\": \"\", # можно и грамматику сделать пустой\n",
    "                            })\n",
    "\n",
    "\n",
    "                lemmatized_sentence.append({\n",
    "                    \"word\": word,\n",
    "                    \"analyses\": lemmatized_words,\n",
    "                })\n",
    "\n",
    "            results.append({\n",
    "                \"sentence\": sentence,\n",
    "                \"lemmatized_words\": lemmatized_sentence,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при обработке предложения '{sentence}': {e}\")\n",
    "            results.append({\n",
    "                \"sentence\": sentence,\n",
    "                \"lemmatized_words\": [],\n",
    "            })\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "       for item in results:\n",
    "            outfile.write(f\"Предложение: {item['sentence']}\\n\")\n",
    "            for word_info in item['lemmatized_words']:\n",
    "                outfile.write(f\"  Слово: {word_info['word']}\\n\")\n",
    "                for analysis in word_info[\"analyses\"]:\n",
    "                    outfile.write(f\"    Токен: {analysis['token']}\\n\")\n",
    "                    outfile.write(f\"      Лемма: {analysis['lemma']}\\n\")\n",
    "                    outfile.write(f\"      Грамматика: {analysis['gramm']}\\n\")\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'output_all.txt'\n",
    "    output_file = 'output_lemmatized_all.txt'\n",
    "    tokenize_and_lemmatize(input_file, output_file)\n",
    "    print(f\"Токенизация и лемматизация завершены. Результаты сохранены в '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a730d-0099-4a21-a4d3-371c9d14fa89",
   "metadata": {},
   "source": [
    "#### **2.2 Разбираемся с нелемматизированным остатком!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d20fcd-2130-4844-855c-98cf9ac7a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_without_lemma(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Файл с not_lemma\n",
    "    Находит и записывает в файл слова без лемм (>= 4 символов) из файла output_lemmatized_all.txt\n",
    "    \"\"\"\n",
    "\n",
    "    words_without_lemma = []\n",
    "    current_word = None\n",
    "    lemma_found = False\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith(\"Слово:\"):\n",
    "                if current_word and not lemma_found and len(current_word) >= 4:\n",
    "                    words_without_lemma.append(current_word)\n",
    "                current_word = line.split(\": \", 1)[1]\n",
    "                lemma_found = False\n",
    "            elif line.startswith(\"Лемма:\") and current_word:\n",
    "                parts = line.split(\": \", 1)\n",
    "                if len(parts) > 1:\n",
    "                    lemma = parts[1].strip()\n",
    "                    if lemma:\n",
    "                         lemma_found = True\n",
    "\n",
    "    if current_word and not lemma_found and len(current_word) >= 4:\n",
    "         words_without_lemma.append(current_word)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word in words_without_lemma:\n",
    "            outfile.write(word + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'output_lemmatized_all.txt'\n",
    "    output_file = 'not_lemma.txt'\n",
    "    find_words_without_lemma(input_file, output_file)\n",
    "    print(f\"Слова без леммы (не короче 4 символов) записаны в файл '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040933fd-daf1-4d6e-900d-38100eedeba7",
   "metadata": {},
   "source": [
    "#### **3.1 Нахождение лемм у остатка по нечёткому соответствию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8fd21-e1f7-4203-8c2c-c09af767cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy uniparser-urmi python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3af4a-1585-4508-977e-95a62db7d49b",
   "metadata": {},
   "source": [
    "очень долгий код далее (примерно 50 мин). Нахождение примерных лемм слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5d506-7b18-4bd2-bd9c-ea521b4c7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fuzzywuzzy import process\n",
    "from uniparser_urmi import UrmiAnalyzer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_all_urmi_lexemes(lexemes_file):\n",
    "    \"\"\"\n",
    "    Читает лексемы из файла словаря урми и возвращает список лексем.\n",
    "    \"\"\"\n",
    "    lexemes = []\n",
    "    with open(lexemes_file, 'r', encoding='utf-8') as infile:\n",
    "      for line in infile:\n",
    "        if line.strip().startswith(\"lex: \"):\n",
    "          lexeme = line.strip().replace('lex: ', '')\n",
    "          lexemes.append(lexeme)\n",
    "    #print(lexemes)\n",
    "    return lexemes\n",
    "\n",
    "def extract_consonants(word):\n",
    "    \"\"\"\n",
    "    Извлекает последовательность согласных из слова.\n",
    "    \"\"\"\n",
    "    return \"\".join(re.findall(r\"[bcdfghjklmnpqrstvwxyz’вƶşçţ]\", word, re.IGNORECASE))\n",
    "\n",
    "def compare_consonant_sequences(word, lexeme):\n",
    "    \"\"\"\n",
    "    Сравнивает последовательности согласных в слове и лексеме с учетом весов.\n",
    "    Возвращает коэффициент похожести (0-100).\n",
    "    \"\"\"\n",
    "    word_cons = extract_consonants(word) # согласные\n",
    "    lexeme_cons = extract_consonants(lexeme)\n",
    "    if not word_cons or not lexeme_cons:\n",
    "        return 0\n",
    "    \n",
    "    total_weight = 0\n",
    "    weighted_match = 0\n",
    "    word_idx = 0\n",
    "    lexeme_idx = 0\n",
    "    \n",
    "    while word_idx < len(word_cons) and lexeme_idx < len(lexeme_cons): # пока не закончилось слово\n",
    "        weight = 1.0 # вес согласной по умолчанию\n",
    "        if word_idx < 2:\n",
    "           weight = 3.0 # вес для первой и второй согласной\n",
    "        elif word_idx < 3:\n",
    "            weight = 2.0 # вес для третьей согласной\n",
    "            \n",
    "        total_weight += weight # общий вес\n",
    "\n",
    "        if word_cons[word_idx] == lexeme_cons[lexeme_idx]: # совпадают ли согласные (сравниваем по индексам согласные)\n",
    "          weighted_match += weight  # сюда добавляются взвешенные совпадения\n",
    "          lexeme_idx += 1\n",
    "\n",
    "        word_idx+=1\n",
    "       \n",
    "    return (weighted_match / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "    \n",
    "def find_best_lemmas(word, urmi_lexemes, limit=10, threshold=60):\n",
    "    \"\"\"\n",
    "    Находит наиболее вероятные леммы для слова, используя нечеткое сравнение\n",
    "    и сравнение последовательности согласных. Возвращает список лемм.\n",
    "    \"\"\"\n",
    "    # 1. Предварительный отбор с помощью fuzzywuzzy\n",
    "    preliminary_matches = process.extract(word, urmi_lexemes, limit=limit)\n",
    "    if not preliminary_matches:\n",
    "        return []\n",
    "\n",
    "    best_matches = []\n",
    "    \n",
    "    # 2. Сравнение согласных и выбор подходящих лемм\n",
    "    for lexeme, _ in preliminary_matches:\n",
    "        score = compare_consonant_sequences(word, lexeme)\n",
    "        if score >= threshold and len(lexeme) >= len(word) / 3: # и если слово не оч короткое\n",
    "            best_matches.append((lexeme, score)) # добавляем пару (лемма, скор)\n",
    "    \n",
    "    return best_matches #возвращаем список\n",
    "\n",
    "\n",
    "def find_missing_lemmas(my_words_file, lexemes_file, output_file, limit=10, threshold=60):\n",
    "    \"\"\"\n",
    "    Находит и предлагает леммы для слов, которые не смог распознать UrmiAnalyzer,\n",
    "    и записывает результаты в файл.\n",
    "    \"\"\"\n",
    "    analyzer = UrmiAnalyzer(mode='nodiacritics')\n",
    "    urmi_lexemes = get_all_urmi_lexemes(lexemes_file)\n",
    "\n",
    "    with open(my_words_file, 'r', encoding='utf-8') as infile, \\\n",
    "            open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word in (line.strip() for line in infile):\n",
    "\n",
    "            if word in urmi_lexemes:  # Проверка полного совпадения\n",
    "                outfile.write(f\"Слово: {word}, Предложенная лемма: {word} (100.0%)\\n\")\n",
    "                continue\n",
    "\n",
    "            analyses = analyzer.analyze_words(word)\n",
    "            has_lemma = False\n",
    "            if analyses:\n",
    "                for ana in analyses:\n",
    "                    if ana.lemma:\n",
    "                        has_lemma = True\n",
    "                        break\n",
    "            if not has_lemma:\n",
    "                best_lemmas = find_best_lemmas(word, urmi_lexemes, limit, threshold)\n",
    "                if best_lemmas:\n",
    "                    outfile.write(f\"Слово: {word}, Предложенные леммы: {', '.join([f'{lex} ({score:.1f}%)' for lex, score in best_lemmas])}\\n\")\n",
    "                else:\n",
    "                    outfile.write(f\"Слово: {word}, Не найдено подходящих лемм\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_words_file = \"not_lemma.txt\"\n",
    "    lexemes_file = \"lexemes.txt\"\n",
    "    output_file = \"prob_lemmatized_not_lemma.txt\"\n",
    "    find_missing_lemmas(my_words_file, lexemes_file, output_file, limit=10, threshold=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b27e3-9a1b-4132-bbf5-1bed4442dfc4",
   "metadata": {},
   "source": [
    "#### **3.2 Расфасовка на файл с леммами и проблемными словами.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9ff9-ce85-47b3-9b93-31a4092ac6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def remove_duplicate_lines(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Читает файл, удаляет дубликаты строк и записывает результат в новый файл.\n",
    "    \"\"\"\n",
    "    unique_lines = set()\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            unique_lines.add(line)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in unique_lines:\n",
    "            outfile.write(line)\n",
    "\n",
    "def count_mismatched_chars(word, lemma):\n",
    "    \"\"\"\n",
    "    Считает количество символов, которые есть в лемме, но нет в слове.\n",
    "    \"\"\"\n",
    "    return sum(1 for char in lemma if char not in word)\n",
    "\n",
    "\n",
    "def choose_best_lemma(word, lemmas_str):\n",
    "    \"\"\"\n",
    "    Выбирает лучшую лемму из списка предложенных.\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for item in lemmas_str.split(\", \"):\n",
    "      match = re.search(r\"(\\w+) \\((\\d+\\.?\\d*)\\%\\)\", item)\n",
    "      if match:\n",
    "        lemmas.append((match.group(1), float(match.group(2))))\n",
    "  \n",
    "    if not lemmas:\n",
    "        return None\n",
    "    \n",
    "    # находим максимальный процент схожести\n",
    "    max_score = max(score for _, score in lemmas)\n",
    "    \n",
    "    # Отбираем леммы с максимальным процентом\n",
    "    best_lemmas = [(lemma, score) for lemma, score in lemmas if score == max_score]\n",
    "\n",
    "    if len(best_lemmas) == 1:\n",
    "        return best_lemmas[0]\n",
    "    \n",
    "    # Если несколько лемм с максимальным процентом, то выбираем с наименьшим числом несовпадающих букв!\n",
    "    best_lemma = min(best_lemmas, key=lambda item: count_mismatched_chars(word, item[0]))\n",
    "    \n",
    "    return best_lemma\n",
    "    \n",
    "def process_lemmatized_file(input_file, all_file, problem_file):\n",
    "    \"\"\"\n",
    "    Читает файл с результатами лемматизации, переносит совпадения в один файл\n",
    "    и проблемы в другой.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "            open(all_file, 'w', encoding='utf-8') as allout, \\\n",
    "            open(problem_file, 'w', encoding='utf-8') as probout:\n",
    "        \n",
    "        for line in infile:\n",
    "            if \"Предложенная лемма:\" in line:\n",
    "                allout.write(line)\n",
    "            elif \"Не найдено подходящих лемм\" in line:\n",
    "                probout.write(line)\n",
    "            elif \"Предложенные леммы:\" in line:\n",
    "                match_word = re.search(r\"Слово: (\\w+),\", line)\n",
    "                if match_word:\n",
    "                    word = match_word.group(1)\n",
    "                    match_lemmas = re.search(r\"Предложенные леммы: (.+)\", line)\n",
    "                    if match_lemmas:\n",
    "                        lemmas_str = match_lemmas.group(1)\n",
    "                        best_lemma_tuple = choose_best_lemma(word, lemmas_str)\n",
    "                        if best_lemma_tuple:\n",
    "                           best_lemma, score = best_lemma_tuple\n",
    "                           if score < 63 and len(word) > 6:\n",
    "                               continue\n",
    "                               #probout.write(f\"Слово: {word}, Предложенная лемма: {best_lemma} ({score:.1f}%) +\\n\")\n",
    "                           else:\n",
    "                               allout.write(f\"Слово: {word}, Предложенная лемма: {best_lemma} ({score:.1f}%)\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"prob_lemmatized_not_lemma.txt\"\n",
    "    unique_file = \"prob_lemmatized_not_lemma_unique.txt\"\n",
    "    all_file = \"prob_lemmatized_all.txt\"\n",
    "    problem_file = \"problem_lemmatized.txt\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "      print(f\"Ошибка: файл {input_file} не найден.\")\n",
    "      exit(1)\n",
    "\n",
    "    # без дубликатов записываем в новый файл\n",
    "    remove_duplicate_lines(input_file, unique_file)\n",
    "\n",
    "    process_lemmatized_file(unique_file, all_file, problem_file)\n",
    "    print(f\"Результаты записаны в {all_file} и {problem_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b5191-1868-4601-ad26-47016e8f7da5",
   "metadata": {},
   "source": [
    "#### **4.1 Сохраняем лемматизированные предложения из отдельных разобранных слов.**\n",
    "   - без стоп-слов (коротких слов)\n",
    "   - без лишних разборов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a9e0e-0b73-417b-b904-e0a6eb4d7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "current_sentence = None\n",
    "current_lemmas = []\n",
    "\n",
    "input_file = 'output_lemmatized_all.txt'\n",
    "output_file = 'output_lemmas_sentences_all_1.txt'\n",
    "prob_lemmatized_all_file = 'prob_lemmatized_all.txt'\n",
    "\n",
    "\n",
    "def find_lemma_in_prob_file(token, prob_lemmatized_all_file):\n",
    "    \"\"\"\n",
    "    Ищет лемму для токена в файле prob_lemmatized_all.txt\n",
    "    \"\"\"\n",
    "    with open(prob_lemmatized_all_file, 'r', encoding='utf-8') as prob_file:\n",
    "        for line in prob_file:\n",
    "            if line.startswith(f\"Слово: {token},\"):\n",
    "               match = re.search(r\"Предложенная лемма: ([\\w\\d]+)\", line)\n",
    "               if match:\n",
    "                   print(match.group(1))\n",
    "                   return match.group(1)\n",
    "    return None\n",
    "    \n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith(\"Предложение:\"):\n",
    "            '''\n",
    "            Если current_sentence не None (т.е предложение уде обработано)\n",
    "            то текущее предложение и его леммы добавляются в список results, \n",
    "            а список current_lemmas очищается для нового предложения.\n",
    "            '''\n",
    "            if current_sentence is not None: \n",
    "                results.append({\"sentence\": current_sentence, \"lemmas\": current_lemmas})\n",
    "                current_lemmas = []\n",
    "            current_sentence = line.split(\": \", 1)[1]\n",
    "\n",
    "            \n",
    "        # иначе обрабатываем предложение:\n",
    "        elif line.startswith(\"Слово:\"):\n",
    "            token = None  # для запоминания токена\n",
    "            lemma_found = False # для запоминания: найдена ли лемма\n",
    "            \n",
    "            # извлекаем токен\n",
    "            token_line = next(infile).strip()  # Читаем следующую строку\n",
    "            if \": \" in token_line:\n",
    "                token = token_line.split(\": \", 1)[1]\n",
    "            \n",
    "            # извлекаем лемму\n",
    "            try:\n",
    "                lemma_line = next(infile).strip()  # читаем следующую строку\n",
    "                if \": \" in lemma_line:\n",
    "                    lemma_candidates = lemma_line.split(\": \", 1)[1].strip().split('/') # лемма первая от слэша\n",
    "                    \n",
    "                    # проверяем наличие леммы\n",
    "                    if lemma_candidates and len(lemma_candidates[0]) > 0:\n",
    "                        first_lemma = lemma_candidates[0]\n",
    "                        if len(first_lemma) >= 4:\n",
    "                            current_lemmas.append(first_lemma)  # берём первую лемму\n",
    "                            lemma_found = True\n",
    "                \n",
    "            except StopIteration: #пропустить, если след строки нет\n",
    "                pass\n",
    "            \n",
    "            if not lemma_found and token and len(token) >= 4:\n",
    "                lemma_from_prob = find_lemma_in_prob_file(token, prob_lemmatized_all_file)\n",
    "                \n",
    "                if lemma_from_prob:\n",
    "                   current_lemmas.append(lemma_from_prob)\n",
    "                else:\n",
    "                   current_lemmas.append(token)\n",
    "\n",
    "   \n",
    "    if current_sentence is not None:\n",
    "        results.append({\"sentence\": current_sentence, \"lemmas\": current_lemmas})\n",
    "    #print(results)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for item in results:\n",
    "        #print(item)\n",
    "        lemma_string = ' '.join(item['lemmas']) \n",
    "        outfile.write(lemma_string.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0d250-3262-4582-8920-4d091adeee2e",
   "metadata": {},
   "source": [
    "**4.2 Перепроверка, чистка итогового файла**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca075e3-7984-4019-ab72-e8c5a00fdc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_lemmas_and_sentences(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Удаляет короткие слова и однословные предложения из файла.\n",
    "    \"\"\"\n",
    "    filtered_lines = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            words = line.strip().split()\n",
    "            filtered_words = [word for word in words if len(word) >= 3]\n",
    "            if len(filtered_words) > 1:\n",
    "                filtered_lines.append(\" \".join(filtered_words) + \"\\n\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(filtered_lines)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'output_lemmas_sentences_all_1.txt'\n",
    "    output_file = 'output_lemmas_sentences_filtered.txt'\n",
    "    \n",
    "    filter_lemmas_and_sentences(input_file, output_file)\n",
    "    print(f\"Результаты записаны в {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaddc05-3e53-4ade-abaf-9712f060d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_unique_words(input_file):\n",
    "    \"\"\"\n",
    "    Сколько уникальных слов\n",
    "    \"\"\"\n",
    "    unique_words = set()\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return len(unique_words)\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'output_lemmas_sentences_filtered.txt'\n",
    "    unique_word_count = count_unique_words(input_file)\n",
    "    print(f\"Количество уникальных слов в файле {input_file}: {unique_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48603fb-0122-4108-998d-59af783724b4",
   "metadata": {},
   "source": [
    "#### **Оцифровка фото**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945469f0-1bd3-4584-a0c2-d39bbf805566",
   "metadata": {},
   "source": [
    "**Распознавание с помощью tesseract**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10eb41-3d1f-449f-99de-d78245381866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_image(image_path, lang='tur', output_file=None):\n",
    "    \"\"\"Распознает текст на изображении с помощью Tesseract.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image, lang=lang)\n",
    "        \n",
    "        if output_file:\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "            print(f\"Текст успешно записан в файл {output_file}\")\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке изображения {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = '/home/kuklelik/Jupyter/ассирийские файлы/npg_кусочки/фото.png'\n",
    "    output_file = '/home/kuklelik/Jupyter/recognized_text.txt'\n",
    "    recognized_text = ocr_image(image_path, 'tur', output_file)\n",
    "    if recognized_text:\n",
    "        print(f\"Распознанный текст:\\n{recognized_text}\")\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def process_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        text = re.sub(r'(?<!\\.)\\n', ' ', text)\n",
    "        text = text.replace('-', '')\n",
    "        text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "        text = re.sub(r'([a-z,İ]{1,2})\\'', r'\\1 ', text.lower())\n",
    "        text = text.replace(\"'\", '').replace('ä', 'a').replace('ö', 'o').replace('ü', 'u').replace('â', 'a')\n",
    "        \n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "        \n",
    "        print(f\"Файл {file_path} обработан успешно.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/home/kuklelik/Jupyter/recognized_text.txt'\n",
    "    process_text(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150cfab-f0e5-44ac-81b3-73b389a1054b",
   "metadata": {},
   "source": [
    "**Создание словаря токенов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b66cad-a482-4ee7-a62f-a2f01e58f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь для сравнивания\n",
    "def create_dictionary(input_file, output_file):\n",
    "    tokens = set()\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip().startswith('Токен:'):\n",
    "                token = line.strip().split(': ')[1]\n",
    "                tokens.add(token)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as dict_file:\n",
    "        for token in tokens:\n",
    "            dict_file.write(token + '\\n')\n",
    "\n",
    "input_file = '/home/kuklelik/Jupyter/output_lemmatized_all.txt'\n",
    "output_file = 'dict_for_tesseract.txt'\n",
    "create_dictionary(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be806604-abca-4237-8225-47251dbabb0d",
   "metadata": {},
   "source": [
    "**Корректировочный код**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4e58e-9e9c-436e-a727-1fa91577043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def load_dictionary(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def correct_text(detected_text, dictionary):\n",
    "    sentences = detected_text.split('\\n')\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) > 3 and any(char in \"oabs\" for char in word):\n",
    "                # Проверка сходства слова с проблемными буквами\n",
    "                similar_length_words = [w for w in dictionary if abs(len(w) - len(word)) <= 1]\n",
    "                if similar_length_words:\n",
    "                    best_match_original = process.extractOne(word, similar_length_words)\n",
    "                    if best_match_original and best_match_original[1] > 90:\n",
    "                        print(f\"Слово '{word}' уже близко к словарю: {best_match_original[0]}\")\n",
    "                        corrected_words.append(best_match_original[0])  # замена на словарное слово\n",
    "                        continue\n",
    "                \n",
    "                #проверяем оба варианта \"g\" на \"q\"\n",
    "                word_with_q = word.replace('g', 'q')\n",
    "                word_with_ə_ь = word.replace('o', 'ə').replace('a', 'ə').replace('s', 'ə').replace('b', 'ь')\n",
    "                word_with_ə_ь_q = word_with_q.replace('o', 'ə').replace('a', 'ə').replace('s', 'ə').replace('b', 'ь')\n",
    "                \n",
    "                # ищем слова в словаре с той же длиной +-1\n",
    "                similar_length_words = [w for w in dictionary if abs(len(w) - len(word)) <= 1]\n",
    "                \n",
    "                if similar_length_words:\n",
    "                    #наиболее близкое слово по расстоянию Левенштейна для обоих вариантов\n",
    "                    best_match_ə_ь = process.extractOne(word_with_ə_ь, similar_length_words)\n",
    "                    best_match_ə_ь_q = process.extractOne(word_with_ə_ь_q, similar_length_words)\n",
    "                    \n",
    "                    if best_match_ə_ь and best_match_ə_ь[1] > 70 and abs(len(word_with_ə_ь) - len(best_match_ə_ь[0])) == 0:\n",
    "                        corrected_word = best_match_ə_ь[0]\n",
    "                        print(\"лучшее ə_ь: \", best_match_ə_ь[0])\n",
    "                    elif best_match_ə_ь_q and best_match_ə_ь_q[1] > 70 and abs(len(word_with_ə_ь_q) - len(best_match_ə_ь_q[0])) == 0:\n",
    "                        corrected_word = best_match_ə_ь_q[0]\n",
    "                        print(\"лучшее ə_ь_q: \", best_match_ə_ь_q[0])\n",
    "                    else:\n",
    "                        corrected_word = word\n",
    "                else:\n",
    "                    corrected_word = word\n",
    "                \n",
    "                #  оставшиеся \"o\" заменяем на \"ə\"\n",
    "                if corrected_word.endswith('o'):\n",
    "                    corrected_word = corrected_word[:-1] + 'ə'\n",
    "                \n",
    "                corrected_words.append(corrected_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        corrected_sentences.append(' '.join(corrected_words))\n",
    "    \n",
    "    return '\\n'.join(corrected_sentences)\n",
    "\n",
    "\n",
    "text_path = 'recognized_text.txt'\n",
    "\n",
    "dictionary_path = 'dict_for_tesseract.txt'\n",
    "\n",
    "dictionary = load_dictionary(dictionary_path)\n",
    "detected_text = load_text(text_path)\n",
    "\n",
    "corrected_text = correct_text(detected_text, dictionary)\n",
    "\n",
    "print(\"Исходный текст:\\n\", detected_text)\n",
    "print(\"Исправленный текст:\\n\", corrected_text)\n",
    "\n",
    "\n",
    "def save_corrected_text(corrected_text, output_file=\"recognized_text.txt\"):\n",
    "    \"\"\"Записывает исправленный текст в файл.\"\"\"\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(corrected_text)\n",
    "        print(f\"Исправленный текст записан в файл: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при записи в файл: {e}\")\n",
    "\n",
    "save_corrected_text(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceab296-d221-404e-8423-ac8903efef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для записи в общий файл разобранного текста\n",
    "import os\n",
    "\n",
    "def append_to_log_file(log_file_path, source_file_path):\n",
    "    try:\n",
    "        with open(source_file_path, 'r', encoding='utf-8') as source_file:\n",
    "            source_content = source_file.read()\n",
    "        \n",
    "        with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(source_content + '\\n\\n')  # Добавляем пустые строки для разделения записей\n",
    "        \n",
    "        print(f\"Содержимое {source_file_path} успешно добавлено в {log_file_path}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл {source_file_path} не найден.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "# Пути к файлам\n",
    "log_file_path = 'recognized_all.txt'\n",
    "source_file_path = 'recognized_text.txt'\n",
    "\n",
    "# Создание файла recognized_all.txt, если он не существует\n",
    "if not os.path.exists(log_file_path):\n",
    "    open(log_file_path, 'w').close()\n",
    "    print(f\"Файл {log_file_path} создан.\")\n",
    "\n",
    "# Дозапись содержимого recognized_text.txt в recognized_all.txt\n",
    "append_to_log_file(log_file_path, source_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a2310-4d70-4261-9a9d-1bb7b10f5f32",
   "metadata": {},
   "source": [
    "**Сравнительный анализ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5c6af-0174-441d-aa92-04e2b69c0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23442e21-8836-48ec-96a7-010cce8435e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "from difflib import Differ\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # Удаляем пунктуацию, пробелы, приводим к нижнему регистру\n",
    "    text = re.sub(r'[^\\w]', '', text).lower().replace(\" \", \"\")\n",
    "    return text\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return [preprocess(line) for line in f.readlines()]\n",
    "\n",
    "text1 = read_file('text1.txt')\n",
    "text2 = read_file('text2.txt')\n",
    "\n",
    "# Сравнение по Левенштейну\n",
    "combined1 = ''.join(text1)\n",
    "combined2 = ''.join(text2)\n",
    "distance = lev.distance(combined1, combined2)\n",
    "similarity = (1 - distance / max(len(combined1), len(combined2))) * 100\n",
    "print(f\"Схожесть после предобработки: {similarity:.1f}%\")\n",
    "\n",
    "# Визуализация различий (на оригинальных строках)\n",
    "with open('text1.txt', 'r', encoding='utf-8') as f1, open('text2.txt', 'r', encoding='utf-8') as f2:\n",
    "    orig1 = f1.readlines()\n",
    "    orig2 = f2.readlines()\n",
    "\n",
    "d = Differ()\n",
    "diff = list(d.compare(orig1, orig2))\n",
    "\n",
    "print(\"\\nОсновные различия в оригинальных текстах:\")\n",
    "for line in diff[:20]:  # Показываем первые 20 различий\n",
    "    if line.startswith('- ') or line.startswith('+ '):\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48416553-4712-45e0-b849-902f3f4f0a11",
   "metadata": {},
   "source": [
    "Схожесть после предобработки: 71.7%\n",
    "\n",
    "Основные различия в оригинальных текстах:\n",
    "- hindvəji dərqul d imperialisti\n",
    "+ Hindvəji dərkul d'imperialisti\n",
    "- (masi̇s nani ildme)\n",
    "- narazbjuta min thumə d inolisnsii\n",
    "+ narazbjuta min huqma d'inglisnji\n",
    "- go hindustan gul um guş ii\n",
    "+ go Hindustan qul juma qus gərvusla\n",
    "- danatb xarajb mtela i̇tixubə xaraja\n",
    "+ danatb xaraјb mtәla l'tixubo xaraјa,\n",
    "- ina dureuaza hindustan ka gi\n",
    "- nəpilvə kodrə tamam gis alma ka\n",
    "+ ijna burzuazija d'Hindustan kә d'la\n",
    "+ nәpilva kәdro tamam l'qiѕ alma d'la\n",
    "- famamta d rbzaja d alma midri qomitet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a41903-6761-45ae-8915-1af6243780b6",
   "metadata": {},
   "source": [
    "тяжёлые буквы:\n",
    "\n",
    "    Ə ə (schwa),\n",
    "    Ţ ţ (T с седилью снизу),\n",
    "    Ş ş (S с седилью снизу),\n",
    "    Ç ç (C с седилью снизу).\n",
    "    Ь ь\n",
    "\n",
    "можно пробовать турецкий (НЕТ Ə ə, Ţ ţ, Ь ь), румынский ( НЕТ Ə ə, Ç ç, Ь ь), албанский (НИчего нет, кроме диакритик каких-то похожих)\n",
    "\n",
    "турецкий:  Ə  распознаёт как \"о, а, е, s\"\n",
    "ь как b\n",
    "\n",
    "Можно попробовать почистить от коротких слов и пр. Токенизировать, лемматизировать. И если леммы не найдётся - попеределывать буквы.  И снова позапускать"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0c8eb-769f-467f-b694-58ea164e230a",
   "metadata": {},
   "source": [
    "### **Для SBOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c16c9-d767-4718-bf42-627e43c596f0",
   "metadata": {},
   "source": [
    "11352 -> 10720 13201 -> 5904\n",
    "\n",
    "в парсере 3643 у меня 3106 + 2798 = 5904 (побочка 537)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99576a-c964-423b-8ce4-fecbc3feb3a2",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675b240-d155-4372-a05d-a362fbd1deb0",
   "metadata": {},
   "source": [
    " 1/ Сначала он создает **словарь**, «обучаясь» на входных текстовых данных, \n",
    " \n",
    " а 2/ затем вычисляет векторное представление слов (**эмбеддинги**).\n",
    " \n",
    " Векторное представление основывается на *контекстной близости*: слова, *встречающиеся в тексте рядом с одинаковыми словами*\n",
    " (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл),\n",
    " в векторном представлении будут иметь *близкие координаты векторов-слов*. Для вычисления близости слов используется **косинусное расстояние между их векторами**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d59fa2-4570-471a-954e-03ff2e951c98",
   "metadata": {},
   "source": [
    "- Сначала прочитайте корпус *построчно* (новая строка = новый документ) и получите список списков, \n",
    "где элементами внешнего списка являются документы, а элементами документа являются слова, которые встречаются в этом документе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87667e-0108-4974-b7fd-537b156381fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращает список документов, где каждый документ представлен как список слов.\n",
    "# Эта функция загружает текстовый корпус из указанного файла и возвращает его в виде списка списков слов. \n",
    "# Каждый элемент внешнего списка соответствует строке в файле.\n",
    "\n",
    "import io\n",
    "\n",
    "def load_corpus(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    documents = []\n",
    "    for line in fin:\n",
    "        documents.append(line.split())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3e59-7b96-44cf-9281-17446d87d21d",
   "metadata": {},
   "source": [
    "- Далее нам нужно сохранить словарь в файл и загрузить его из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fd419-2408-4f6f-9b87-cf153acf536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта функция сохраняет словарь (где ключи - слова, а значения - их векторные представления) в файл.\n",
    "# Это необходимо, чтобы сохранить результаты обучения модели.\n",
    "def save_dictionary(fname, dictionary, args):\n",
    "    length, dimension = args #длина словаря и размерность векторов\n",
    "    fin = io.open(fname, 'w', encoding='utf-8')\n",
    "    fin.write('%d %d\\n' % (length, dimension))\n",
    "    for word in dictionary:\n",
    "        fin.write('%s %s\\n' % (word, ' '.join(map(str, dictionary[word]))))\n",
    "\n",
    "# Эта функция читает словарь из файла, созданного с помощью функции save_dictionary. \n",
    "# Это позволяет загрузить ранее обученную модель.\n",
    "def load_dictionary(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    length, dimension = map(int, fin.readline().split())\n",
    "    dictionary = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        dictionary[tokens[0]] = map(float, tokens[1:])\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbee62e-77e6-4311-8641-e4eb093b336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_corpus('output_lemmas_sentences_filtered.txt')\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e84729-dd43-450b-91e4-be6fde8b9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe9254-a86a-48fb-b533-b66881a256a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('pulxənə', topn = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8f899-b7d1-4980-b9be-606c1b8f0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary('urmi_dictionary.txt', dictionary, (len(dictionary), dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3b798-09b9-48bc-a260-2e31e757231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dictionary = load_dictionary('urmi_dictionary.txt')\n",
    "len(dictionary) == len(loaded_dictionary)\n",
    "# сравнивает длину словаря, только что созданного на основе модели `Word2Vec`, со словарём, загруженным из файла.\n",
    "# результатом будет `True`, если длина словарей совпадает, и `False` если не совпадает. Это сделано для проверки того,\n",
    "# что длина обоих словарей совпадает."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e07885-86d6-4ff8-9524-d1fa032ce8b3",
   "metadata": {},
   "source": [
    "    13201 8\n",
    "    həvi -2.8979242 -2.632021 2.0544846 1.2907332 0.3593198 -0.72183436 4.841792 0.006001376\n",
    "    вitə -3.1749682 -2.6964538 2.1477616 1.1134115 0.34639588 -0.4612211 4.59261 -0.19204788\n",
    "    ijli -3.2210557 -2.9326 1.9871407 1.1401004 0.71539384 -0.6838911 5.3129373 -0.15462482\n",
    "    gənə -3.093706 -2.983472 2.121494 1.4011399 0.4959302 -0.68010485 5.025494 -0.027100356"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
